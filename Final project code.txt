{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red109\green109\blue109;\red32\green32\blue32;\red191\green100\blue38;
\red153\green168\blue186;\red152\green54\blue29;\red88\green118\blue71;\red117\green114\blue185;\red86\green132\blue173;
}
{\*\expandedcolortbl;;\csgenericrgb\c42745\c42745\c42745;\csgenericrgb\c12549\c12549\c12549;\csgenericrgb\c74902\c39216\c14902;
\csgenericrgb\c60000\c65882\c72941;\csgenericrgb\c59608\c21176\c11373;\csgenericrgb\c34510\c46275\c27843;\csgenericrgb\c45882\c44706\c72549;\csgenericrgb\c33725\c51765\c67843;
}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs26 \cf2 \cb3 # Importing Libraries\
\cf4 import \cf5 pandas \cf4 as \cf5 pd\
\cf4 import \cf5 numpy \cf4 as \cf5 np\
\cf4 import \cf5 seaborn \cf4 as \cf5 sns\
\
\cf4 import \cf5 warnings\
warnings.simplefilter(\cf6 action\cf5 =\cf7 'ignore'\cf4 , \cf6 category\cf5 =\cf8 FutureWarning\cf5 )\
\
\cf2 # Libraries for Visualization Purposes\
\cf4 import \cf5 matplotlib.pyplot \cf4 as \cf5 plt\
\
\
\cf4 from \cf5 sklearn.preprocessing \cf4 import \cf5 StandardScaler\
\cf4 from \cf5 sklearn.model_selection \cf4 import \cf5 train_test_split\
\cf4 from \cf5 sklearn.linear_model \cf4 import \cf5 LinearRegression\
\cf4 from \cf5 sklearn \cf4 import \cf5 linear_model\
\cf4 import \cf5 statsmodels.api \cf4 as \cf5 sm\
\cf4 from \cf5 sklearn \cf4 import \cf5 model_selection\
\cf4 from \cf5 sklearn.preprocessing \cf4 import \cf5 MinMaxScaler\
\cf4 from \cf5 sklearn.metrics \cf4 import \cf5 mean_squared_error\
\cf4 from \cf5 sklearn.metrics \cf4 import \cf5 r2_score\
\cf4 from \cf5 sklearn \cf4 import \cf5 preprocessing\
\cf4 from \cf5 sklearn.preprocessing \cf4 import \cf5 scale\
\cf4 from \cf5 sklearn.decomposition \cf4 import \cf5 PCA\
\cf4 from \cf5 scipy.signal \cf4 import \cf5 savgol_filter\
\cf4 from \cf5 sklearn.model_selection \cf4 import \cf5 cross_val_predict\
\
\cf2 # Loading the Clean Churn Dataset\
\cf5 real_estate_df = pd.read_csv(\cf7 '/Users/bia/Desktop/all_v2.csv'\cf5 )\
\
\cf2 #Dataset Size with SHAPE\
\cf8 print\cf5 (real_estate_df.shape)\
\
\cf2 #Dataset columns\
\cf8 print\cf5 (real_estate_df.columns)\
\
\cf2 #Dataset Info\
\cf8 print\cf5 (real_estate_df.info)\
\
\cf2 #Dataset Columns Types\
\cf8 print\cf5 (real_estate_df.dtypes)\
\
\cf2 #Basic Stats of the data\
\cf8 print\cf5 (real_estate_df.describe())\
\
\cf2 #Removing some unnecessary categorical data\
\cf5 real_estate_df = real_estate_df.drop(\cf6 columns\cf5 =[\cf7 'date'\cf4 , \cf7 'time'\cf5 ])\
\
\cf2 #Verifying that the dataset only has numerical columns:\
#Dataset Columns Types\
\cf8 print\cf5 (real_estate_df.dtypes)\
\
\cf2 #Histogram for Numerical Data\
# dataset = real_estate_df[['price', 'geo_lat', 'geo_lon', 'region', 'building_type', 'level',\
#                     'levels','rooms', 'area', 'kitchen_area', 'object_type']]\
# fig = plt.figure(figsize=(15, 12))\
# plt.suptitle('Histograms - Numerical Data \\n', horizontalalignment="center", fontstyle="normal", fontsize=24,\
#              fontfamily="sans-serif")\
# for i in range(dataset.shape[1]):\
#     plt.subplot(6, 3, i + 1)\
#     f = plt.gca()\
#     f.set_title(dataset.columns.values[i])\
#     vals = np.size(dataset.iloc[:, i].unique())\
#     if vals >= 100:\
#         vals = 100\
#     plt.hist(dataset.iloc[:, i], bins=vals, color='#ec838a')\
# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\
# plt.show()\
\
\
# # Create a scatterplot to get an idea of correlations between potentially related variables\
# sns.scatterplot(x=russia_df['rooms'], y=russia_df['price'], color='blue')\
# plt.show()\
\
# #Correlation Matrix\
# sns.heatmap(real_estate_df.corr(), annot = True)\
# plt.show()\
\
##Geo_lon and Region are highly correlated --> dropping geo_lon\
#real_estate_df = real_estate_df.drop('geo_lon', axis = 1)\
\
\
# #Plotting features and Price correlations in descending order\
# real_estate_df.corr()['price'].sort_values(ascending = False).plot(kind ='bar', figsize = (10, 5), color = 'Red')\
# plt.title('Correlation Of Variables With Price', fontsize = 20, fontweight = 'bold')\
# plt.xticks(fontsize = 10, fontweight = 'bold')\
# plt.yticks(fontweight = 'bold', fontsize = 10)\
# plt.show()\
\
\
############################################# FIX IT ##################################################################\
# Scatter Matrix\
# real_estate_num = real_estate_df[['price', 'geo_lat', 'geo_lon', 'region', 'building_type', 'level',\
#                     'levels','rooms', 'area', 'kitchen_area', 'object_type']]\
#\
#\
# scatter_matrix = pd.plotting.scatter_matrix(real_estate_df, figsize = [15,15], diagonal = "kde", color = "magenta")\
#\
# for ax in scatter_matrix.ravel():\
#     ax.set_xlabel(ax.get_xlabel(), fontsize = 10, rotation = 90)\
#     ax.set_ylabel(ax.get_ylabel(), fontsize = 10, rotation = 0)\
# plt.title('Scatter Matrix')\
# plt.show()\
############################################# FIX IT ##################################################################\
\
\
\
# # Create Seaborn boxplots for continuous variables\
# sns.boxplot('price', data = real_estate_df)\
# plt.show()\
# print("Prices are: ", real_estate_df.price.describe())\
\
# Since price has outliers, let's remove them\
\cf5 outliers_high = real_estate_df[real_estate_df[\cf7 'price'\cf5 ] >= \cf9 300000000\cf5 ].index\
real_estate_df.drop(outliers_high\cf4 , \cf6 inplace \cf5 = \cf4 True\cf5 )\
\
outliers_low = real_estate_df[real_estate_df[\cf7 'price'\cf5 ] < \cf9 10000\cf5 ].index\
real_estate_df.drop(outliers_low\cf4 , \cf6 inplace \cf5 = \cf4 True\cf5 )\
\
\cf2 # # Create Seaborn boxplots for continuous variables\
# sns.boxplot('price', data = real_estate_df)\
# plt.show()\
# print("Prices are: ", real_estate_df.price.describe())\
\
#Finding missing values in my dataset\
\cf5 real_estate_df.isnull().any(\cf6 axis\cf5 =\cf9 1\cf5 )\
null_values = real_estate_df.isna().any()\
\cf8 print\cf5 (null_values)\
\
\cf2 #Target is the price (Y)\
\cf5 y_data = real_estate_df.price.values\
\
\cf2 #Remove price from remaining features\
\cf5 X_data = real_estate_df.drop(\cf7 'price'\cf4 , \cf6 axis \cf5 = \cf9 1\cf5 )\
\
\cf2 ######PCA ANALYSIS#########\
\cf5 pca = PCA()\
\cf2 # real_estate_normalized = (real_estate_df - real_estate_df.mean()) / real_estate_df.std()\
\cf5 Xstd = StandardScaler().fit_transform(X_data)\
Xreg = pca.fit_transform(Xstd)\
\
regression = linear_model.LinearRegression()\
\
\cf2 #Splitting the data: Training and Testing\
\cf5 X_train\cf4 , \cf5 X_test\cf4 , \cf5 y_train\cf4 , \cf5 y_test = train_test_split(Xreg\cf4 , \cf5 y_data\cf4 , \cf6 test_size \cf5 = \cf9 0.3\cf4 , \cf6 random_state \cf5 = \cf9 0\cf5 )\
\
\cf2 #Fitting the model\
\cf5 model = regression.fit(X_train\cf4 , \cf5 y_train)\
\
r_squared = model.score(X_train\cf4 , \cf5 y_train)\
\cf8 print\cf5 (\cf7 'Coefficient of Determination:'\cf4 , \cf5 r_squared)\
\
\cf2 #Creating predictions\
\cf5 y_hat_train = model.predict(X_train)\
y_pred = model.predict(X_test)\
\
\cf8 print\cf5 (\cf7 'Predicted Response:'\cf4 , \cf5 y_pred\cf4 , \cf6 sep\cf5 =\cf7 '\cf4 \\n\cf7 '\cf5 )\
\cf8 print\cf5 (\cf7 'Y Training:'\cf4 , \cf5 y_hat_train\cf4 , \cf6 sep\cf5 =\cf7 '\cf4 \\n\cf7 '\cf5 )\
\
\cf8 print\cf5 (\cf7 'Intercept:'\cf4 , \cf5 model.intercept_)\
\cf8 print\cf5 (\cf7 'Coefficients:'\cf4 , \cf5 model.coef_)\
\cf8 print\cf5 (\cf7 'MSE: %.2f' \cf5 % mean_squared_error(y_test\cf4 , \cf5 y_pred))\
\cf8 print\cf5 (\cf7 'R^2: %.2f' \cf5 % r2_score(y_test\cf4 , \cf5 y_pred))\
\
\cf2 #OLS Model Summary\
\cf5 real_estate_df[\cf7 'intercept'\cf5 ] = \cf9 1\
\cf5 lm_real_estate = sm.OLS(real_estate_df[\cf7 'price'\cf5 ]\cf4 , \cf5 real_estate_df[[\cf7 'geo_lat'\cf4 , \cf7 'region'\cf4 , \cf7 'building_type'\cf4 ,\
                                                                 \cf7 'level'\cf4 , \cf7 'levels'\cf4 , \cf7 'rooms'\cf4 , \cf7 'area'\cf4 , \cf7 'kitchen_area'\cf4 ,\
                                                                 \cf7 'object_type'\cf4 ,\cf7 'intercept'\cf5 ]]).fit()\
\cf8 print\cf5 (lm_real_estate.summary())\
\
\
\
\cf2 # # Min-Max scaling object\
# mm = MinMaxScaler()\
# X_data = pd.DataFrame(mm.fit_transform(X_data))\
\
# #Saving dataframe column titles to list\
# cols = X_data.columns\
\
\
# X_reduced = pca.fit_transform(scale(X_data))\
#\
# #Explained Variance\
# plt.plot(pca.explained_variance_ratio_)\
# plt.xlabel('Number of Components')\
# plt.ylabel('Explained Variance')\
# plt.show()\
#\
# print('Variance of each component is: ', pca.explained_variance_ratio_)\
#\
# #Extract the eigenvalues\
# cov_matrix = np.dot(real_estate_normalized.T, real_estate_normalized) / real_estate_df.shape[0]\
# eigenvalues = [np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)) for eigenvector in pca.components_]\
#\
# # Plot the eigenvalues\
# plt.plot(eigenvalues)\
# plt.xlabel('Number of Components')\
# plt.ylabel('Eigenvalue')\
# plt.show()\
#\
#\
# #Training and Test data split 70/30\
# # X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3, random_state = 0)\
# X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_data, test_size = 0.3, random_state = 0)\
#\
# #Creating the model\
# model = LinearRegression().fit(X_train, y_train)\
#\
#\
# r_squared = model.score(X_train, y_train)\
# print('Coefficient of Determination:', r_squared)\
#\
# #Creating predictions\
# y_hat_train = model.predict(X_train)\
# y_pred = model.predict(X_test)\
#\
# print('Predicted Response:', y_pred, sep='\\n')\
# print('Y Training:', y_hat_train, sep='\\n')\
#\
# print('Intercept:', model.intercept_)\
# print('Coefficients:', model.coef_)\
# print('MSE: %.2f' % mean_squared_error(y_test, y_pred))\
# print('R^2: %.2f' % r2_score(y_test, y_pred))\
#\
# #OLS Model Summary\
# real_estate_df['intercept'] = 1\
# lm_real_estate = sm.OLS(real_estate_df['price'], real_estate_df[['geo_lat', 'region', 'building_type',\
#                                                                  'level', 'levels', 'rooms', 'area', 'kitchen_area',\
#                                                                  'object_type','intercept']]).fit()\
# print(lm_real_estate.summary())\
\
\
\
\
# # Normalize the data\
# real_estate_normalized = (real_estate_df - real_estate_df.mean()) / real_estate_df.std()\
# pca = PCA(n_components = real_estate_normalized.shape[1])\
# print(real_estate_normalized.shape[1])\
\
\
#\
# pcs_names = []\
# for i, col in enumerate(real_estate_df.columns):\
#     pcs_names.append('PC' + str(i + 1))\
# print(pcs_names)\
\
# pca.fit(real_estate_normalized)\
# real_estate_pca = pd.DataFrame(pca.transform(real_estate_normalized), columns = pcs_names)\
\
# plt.plot(pca.explained_variance_ratio_)\
# plt.xlabel('Number of Components')\
# plt.ylabel('Explained Variance')\
# plt.show()\
#\
# print('Variance of each component is: ', pca.explained_variance_ratio_)\
\
# #Extract the eigenvalues\
# cov_matrix = np.dot(real_estate_normalized.T, real_estate_normalized) / real_estate_df.shape[0]\
# eigenvalues = [np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)) for eigenvector in pca.components_]\
#\
# # Plot the eigenvalues\
# plt.plot(eigenvalues)\
# plt.xlabel('Number of Components')\
# plt.ylabel('Eigenvalue')\
# plt.show()\
#\
# print(cov_matrix)\
# #print(eigenvector)\
# print(eigenvalues)\
#\
# #Select the fewest components\
# for pc, var in zip(pcs_names, np.cumsum(pca.explained_variance_ratio_)):\
#     print(pc, var)\
#\
# #Creating Rotation\
# rotation = pd.DataFrame(pca.components_.T, columns = pcs_names, index = real_estate_df.columns)\
# print('Rotation Matrix is: ', rotation)\
\
}